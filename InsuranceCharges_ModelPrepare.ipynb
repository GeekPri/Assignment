{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7f78bb38-6ac7-4fa5-b6c9-77739297d113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",,,,,,,,,,,,1.Multiple Linear Regression Start,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "===================================\n",
      "===================================\n",
      "+++++++++++++++\n",
      "===================================\n",
      "===================================\n",
      "+++++++++++++++\n",
      "===================================\n",
      "===================================\n",
      "+++++++++++++++\n",
      "===================================\n",
      "===================================\n",
      "+++++++++++++++\n",
      ",,,,,,,,,,,,,,,,,1.Multiple Linear Regression Complete,,,,,,,,,,,,,,,,,,,,\n",
      ",,,,,,,,,,,,2.Scalar Vector Model (SVM) --Start,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "===================================\n",
      "===================================\n",
      "===================================\n",
      ",,,,,,,,,,,,,,,,,2.Scalar Vector Model (SVM) Complete,,,,,,,,,,,,,,,,,,,,\n",
      "r2_score for the 0.886 for DT 0.8863459882523649\n",
      "----------------------------\n",
      "DT resultPredictedInsuranceCharges 19, 27, 0, 0, 0 [1748.774]\n",
      "DT resultPredictedInsuranceCharges1 19, 26, 0, 0, 0  [1748.774]\n",
      ",,,,,,,,,,,,3.Decision Tree--Start,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "===================================\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "===================================\n",
      "+++++++++++++++\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "===================================\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "===================================\n",
      "+++++++++++++++\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "===================================\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "===================================\n",
      "+++++++++++++++\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "===================================\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "=======\n",
      "===================================\n",
      "+++++++++++++++\n",
      ",,,,,,,,,,,,,,,,,3.Decision Tree Complete,,,,,,,,,,,,,,,,,,,,\n",
      ",,,,,,,,,,,,4.Random Forest--Start,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "\n",
      "===================================\n",
      "+++++++++++++++\n",
      ",,,,,,,,,,,,,,,,, 4.Random Forest -- Complete,,,,,,,,,,,,,,,,,,,,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mpriy\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\mpriy\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score for the 0.89 0.8914717067507086\n",
      "----------------------------\n",
      "RF resultPredictedInsuranceCharges 19, 27, 0, 0, 0 [1908.72829425]\n",
      "RF resultPredictedInsuranceCharges1 19, 26, 0, 0, 0 : [1904.26720087]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mpriy\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\mpriy\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Data Collection\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"insurance_pre.csv\")\n",
    "\n",
    "#this dataset has 5 input(with 2 non-numerical value) and 1 output\n",
    " \n",
    "# Data preprocessing\n",
    "#convert the classification(non numerical) data to Numberical\n",
    "dataset= pd.get_dummies(dataset, drop_first= True) # removes the duplicate infering column\n",
    "\n",
    "#Identify the Input and Output\n",
    "independent = dataset[['age', 'bmi', 'children', 'sex_male', 'smoker_yes']]\n",
    "dependent = dataset['charges']\n",
    "\n",
    "#split to train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(independent, dependent, test_size= 0.3, random_state= 0)\n",
    "\n",
    "#the output is Numeric - so this is Regression, ML Supervised Learning as input and output are well defined\n",
    "#build model - more than one input so cannot be Simple linear Regression\n",
    "#possible build models = 1.Multiple Linear Regression or 2.Scalar Vector Model (SVM) or 3.Decision Tree or 4.Random Forest\n",
    "\n",
    "\n",
    "##################1.Multiple Linear Regression #################\n",
    "#possible Hyper tuning params fit_intercept=True, copy_X=True, n_jobs=None, positive=False\n",
    "from sklearn.linear_model import LinearRegression\n",
    "def calculateModelR2Score_MLR(fit_intercept_V, copy_X_V, n_jobs_V, positive_V):\n",
    "    regressor = LinearRegression(fit_intercept=fit_intercept_V, copy_X=copy_X_V, n_jobs=n_jobs_V, positive=positive_V)\n",
    "    regressor = regressor.fit(X_train, Y_train)\n",
    "    \n",
    "    #test using the test data\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    \n",
    "    # evalulate metrics R2 score\n",
    "    from sklearn.metrics import r2_score\n",
    "    r_score = r2_score(Y_test, y_pred)\n",
    "    print(\"fit_intercept:\", fit_intercept_V)\n",
    "    print(\"copy_X:\", copy_X_V)\n",
    "    print(\"n_jobs:\", n_jobs_V)\n",
    "    print(\"positive:\", positive_V)\n",
    "    print(\"r2_score\", r_score)\n",
    "    print(\"----------------------------\") \n",
    "    \n",
    "fit_intercept_List = [True, False]\n",
    "copy_X_List = [False, True]\n",
    "n_jobs_List = [0] #,1,2]\n",
    "positive_List = [False, True]\n",
    "\n",
    "print(\",,,,,,,,,,,,1.Multiple Linear Regression Start,,,,,,,,,,,,,,,,,,,,,,,,,\")\n",
    "for fit_intercept_V in fit_intercept_List:\n",
    "    for copy_X_V in copy_X_List:\n",
    "        for n_jobs_V in n_jobs_List:\n",
    "            for positive_V in positive_List:\n",
    "               # calculateModelR2Score_MLR(fit_intercept_V, copy_X_V, n_jobs_V, positive_V)\n",
    "                print(\"===================================\")\n",
    "        print(\"+++++++++++++++\")\n",
    "print(\",,,,,,,,,,,,,,,,,1.Multiple Linear Regression Complete,,,,,,,,,,,,,,,,,,,,\")\n",
    "\n",
    "##################2.Scalar Vector Model (SVM) #################\n",
    "#possible Hyper tuning params: sklearn.svm.SVR(*, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n",
    "from sklearn.svm import SVR\n",
    "def calculateModelR2Score_SVM(kernel_V, degree_V, gamma_V, C_V, epsilon_V):\n",
    "    regressor = SVR(kernel=kernel_V, degree=degree_V, gamma=gamma_V, C=C_V, epsilon=epsilon_V )\n",
    "    regressor = regressor.fit(X_train, Y_train)\n",
    "    \n",
    "    #test using the test data\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    \n",
    "    # evalulate metrics R2 score\n",
    "    from sklearn.metrics import r2_score\n",
    "    r_score = r2_score(Y_test, y_pred)\n",
    "    \n",
    "    print(\"kernel_V\", kernel_V) \n",
    "    print(\"degree_V\", degree_V)\n",
    "    print(\"gamma_V \", gamma_V)\n",
    "    print(\"C_V \", C_V)\n",
    "    print(\"epsilon_V \", epsilon_V)\n",
    "    print(\"r2_score\", r_score)\n",
    "    print(\"----------------------------\") \n",
    "\n",
    "kernel_List= ['rbf', 'linear', 'poly']\n",
    "C_List= [1] #0.1, 1, 10, 100]\n",
    "gamma_List= [1] #['scale', 'auto', 0.01, 0.1, 1]\n",
    "epsilon_List= [0.001] #, 0.01, 0.1, 1.0]\n",
    "degree_List= [2] #, 3, 4]  \n",
    "\n",
    "print(\",,,,,,,,,,,,2.Scalar Vector Model (SVM) --Start,,,,,,,,,,,,,,,,,,,,,,,,,\")\n",
    "for kernel_V in kernel_List:\n",
    "    for degree_V in degree_List:\n",
    "        for gamma_V in gamma_List:\n",
    "                    for C_V in C_List:\n",
    "                        for epsilon_V in epsilon_List:\n",
    "                                  #          calculateModelR2Score_SVM(kernel_V, degree_V, gamma_V, C_V, epsilon_V)  \n",
    "                            print(\"===================================\")\n",
    "   # print(\"+++++++++++++++\")\n",
    "print(\",,,,,,,,,,,,,,,,,2.Scalar Vector Model (SVM) Complete,,,,,,,,,,,,,,,,,,,,\")\n",
    "\n",
    "##################3.Decision Tree #################\n",
    "#possible Hyper tuning params: DecisionTreeRegressor(*, criterion='squared_error', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, ccp_alpha=0.0, monotonic_cst=None)[source]\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "def calculateModelR2Score_DecisionTree(criterion_V, splitter_V, max_depth_V, min_samples_split_V, min_samples_leaf_V):\n",
    "    regressor = DecisionTreeRegressor(criterion=criterion_V, splitter=splitter_V, max_depth=max_depth_V, min_samples_split=min_samples_split_V, min_samples_leaf=min_samples_leaf_V) #, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, ccp_alpha=0.0, monotonic_cst=None)[source]\n",
    "\n",
    "    regressor = regressor.fit(X_train, Y_train)\n",
    "    \n",
    "    #test using the test data\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    \n",
    "    # evalulate metrics R2 score\n",
    "    from sklearn.metrics import r2_score\n",
    "    r_score = r2_score(Y_test, y_pred)\n",
    "    if(r_score > 0.886):\n",
    "        print(\"criterion_V\", criterion_V) \n",
    "        print(\"splitter_V\", splitter_V)\n",
    "        print(\"max_depth_V \", max_depth_V)\n",
    "        print(\"min_samples_split_V \", min_samples_split_V)\n",
    "        print(\"min_samples_leaf_V \", min_samples_leaf_V)\n",
    "        print(\"r2_score\", r_score)\n",
    "        print(\"----------------------------\") \n",
    "\n",
    "criterion_List = [\"squared_error\", \"friedman_mse\", \"absolute_error\", \"poisson\"]\n",
    "splitter_List = [\"best\", \"random\"]\n",
    "max_depth_List = [None, 3, 5, 10, 20]\n",
    "min_samples_split_List = [2, 5, 10, 20, 50]\n",
    "min_samples_leaf_List = [1, 5, 10, 20, 50]\n",
    "# min_weight_fraction_leaf_List = [0.0, 0.01, 0.05, 0.1]\n",
    "# max_features_List = [None, \"auto\", \"sqrt\", \"log2\"]\n",
    "# random_state_List = [None, 0, 42, 100]  # Used for reproducibility\n",
    "# max_leaf_nodes_List = [None, 10, 50, 100, 200]\n",
    "# min_impurity_decrease_List = [0.0, 0.001, 0.01, 0.1]\n",
    "# ccp_alpha_List = [0.0, 0.001, 0.01, 0.1]\n",
    "\n",
    "\n",
    "#running for the highest score 0.886\n",
    "regressor = DecisionTreeRegressor(criterion=\"absolute_error\", splitter=\"best\", max_depth=None, min_samples_split=20, min_samples_leaf=5) #, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, ccp_alpha=0.0, monotonic_cst=None)[source]\n",
    "regressor = regressor.fit(X_train, Y_train)\n",
    "    \n",
    "#test using the test data\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# evalulate metrics R2 score\n",
    "from sklearn.metrics import r2_score\n",
    "r_score = r2_score(Y_test, y_pred)             \n",
    "\n",
    "print(\"r2_score for the 0.886 for DT\", r_score)\n",
    "print(\"----------------------------\") \n",
    "\n",
    "import pickle\n",
    "filename_DT = \"finalized_InsuranceChargePrediction_DT_Model.sav\"\n",
    "pickle.dump(regressor, open(filename_DT, \"wb\"))\n",
    "\n",
    "# test the model with the saved model file\n",
    "\n",
    "loaded_model_DT = pickle.load(open(\"finalized_InsuranceChargePrediction_DT_Model.sav\", \"rb\"))\n",
    "resultPredictedInsuranceCharges = loaded_model_DT.predict([ [19, 27, 0, 0, 0]]) # 1783.4740471\n",
    "print (\"DT resultPredictedInsuranceCharges 19, 27, 0, 0, 0\" , resultPredictedInsuranceCharges)\n",
    "resultPredictedInsuranceCharges1 = loaded_model_DT.predict([ [19, 26, 0, 0, 0]]) # 16824.917100\n",
    "print (\"DT resultPredictedInsuranceCharges1 19, 26, 0, 0, 0 \" , resultPredictedInsuranceCharges1)\n",
    "\n",
    "\n",
    "print(\",,,,,,,,,,,,3.Decision Tree--Start,,,,,,,,,,,,,,,,,,,,,,,,,\")\n",
    "for criterion_V in criterion_List :\n",
    "    for splitter_V in splitter_List:\n",
    "        for max_depth_V in max_depth_List:\n",
    "                    for min_samples_split_V in min_samples_split_List:\n",
    "                        for min_samples_leaf_V in min_samples_leaf_List:\n",
    "                            print(\"=======\")#calculateModelR2Score_DecisionTree(criterion_V, splitter_V, max_depth_V, min_samples_split_V, min_samples_leaf_V)\n",
    "        print(\"===================================\")\n",
    "    print(\"+++++++++++++++\")\n",
    "print(\",,,,,,,,,,,,,,,,,3.Decision Tree Complete,,,,,,,,,,,,,,,,,,,,\")\n",
    "\n",
    "\n",
    "##################4.Random Forest #################\n",
    "#possible Hyper tuning params: class sklearn.ensemble.RandomForestRegressor(n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "def calculateModelR2Score_RF(n_estimators_Value, criterion_value, max_depth_Value, min_samples_split_Value, min_samples_leaf_Value, max_features_Value, bootstrap_Value, oob_score_Value, random_state_Value):\n",
    "    regressor = RandomForestRegressor(n_estimators=n_estimators_Value, criterion=criterion_value, max_depth=max_depth_Value,   min_samples_split=min_samples_split_Value, min_samples_leaf=min_samples_leaf_Value, min_weight_fraction_leaf=0.0, max_features=max_features_Value, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=bootstrap_Value, oob_score=oob_score_Value, n_jobs=None, random_state=random_state_Value, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "    regressor = regressor.fit(X_train, Y_train)\n",
    "    #test using the test data\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    # evalulate metrics R2 score\n",
    "    from sklearn.metrics import r2_score\n",
    "    r_score = r2_score(Y_test, y_pred)\n",
    "    if(r_score > 0.891):\n",
    "        print(\"n_estimators_Value\", n_estimators_Value) \n",
    "        print(\"criterion_Value\", criterion_value) \n",
    "        print(\"max_depth_Value\", max_depth_Value) \n",
    "        print(\"min_samples_split_Value\", min_samples_split_Value)  \n",
    "        print(\"min_samples_leaf_Value\", min_samples_leaf_Value) \n",
    "        print(\"max_features_Value\", max_features_Value) \n",
    "        print(\"bootstrap_Value\", bootstrap_Value) \n",
    "        print(\"oob_score_Value\", oob_score_Value) \n",
    "        print(\"random_state_Value\", random_state_Value)\n",
    "        print(\"r2_score\", r_score)\n",
    "        print(\"----------------------------\") \n",
    "n_estimators_List = [200] #100, 200] #, 300, 500]  # Number of trees\n",
    "criterion_List = ['absolute_error']#'squared_error', 'absolute_error', 'friedman_mse']\n",
    "max_depth_List = [None, 10] #, 20, 30, 50]  # Maximum depth of trees\n",
    "min_samples_split_List = [2, 5] #, 10]  # Minimum samples required to split a node\n",
    "min_samples_leaf_List = [1, 2, 4]#, 10]  # Minimum samples required at each leaf node\n",
    "max_features_List = ['sqrt', 'log2', None]  # Number of features to consider at each split\n",
    "bootstrap_List = [True] #, False]  # Whether bootstrap samples are used\n",
    "oob_score_List = [True, False]  # Whether to use out-of-bag samples for scoring\n",
    "random_state_List = [30, 42, 50]  # For reproducibility\n",
    "\n",
    "\n",
    "print(\",,,,,,,,,,,,4.Random Forest--Start,,,,,,,,,,,,,,,,,,,,,,,,,\")\n",
    "for n_estimators_Value in n_estimators_List :\n",
    "    for criterion_value in criterion_List:\n",
    "        for max_depth_Value in max_depth_List:\n",
    "                    for min_samples_split_Value in min_samples_split_List:\n",
    "                        for min_samples_leaf_Value in min_samples_leaf_List:\n",
    "                            for max_features_Value in max_features_List:\n",
    "                                        for bootstrap_Value in bootstrap_List:\n",
    "                                            for oob_score_Value in oob_score_List:\n",
    "                                                for random_state_Value in random_state_List:\n",
    "                                                   print(\"\")# calculateModelR2Score_RF(n_estimators_Value, criterion_value, max_depth_Value, min_samples_split_Value, min_samples_leaf_Value, max_features_Value, bootstrap_Value, oob_score_Value, random_state_Value)\n",
    "                                                print(\"===================================\")\n",
    "                                            pass\n",
    "                                        pass\n",
    "                            pass\n",
    "                        pass\n",
    "                    pass\n",
    "        pass\n",
    "    print(\"+++++++++++++++\")\n",
    "print(\",,,,,,,,,,,,,,,,, 4.Random Forest -- Complete,,,,,,,,,,,,,,,,,,,,\")\n",
    "\n",
    "#running for the highest score 0.89\n",
    "regressor = RandomForestRegressor(n_estimators=200,  criterion= 'absolute_error', max_depth=10,   min_samples_split=2, min_samples_leaf=4, min_weight_fraction_leaf=0.0, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=True, n_jobs=None, random_state=42, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "regressor = regressor.fit(X_train, Y_train)\n",
    "#test using the test data\n",
    "y_pred = regressor.predict(X_test)\n",
    "# evalulate metrics R2 score\n",
    "from sklearn.metrics import r2_score\n",
    "r_score = r2_score(Y_test, y_pred)                              \n",
    "\n",
    "print(\"r2_score for the 0.89\", r_score)\n",
    "print(\"----------------------------\") \n",
    "\n",
    "import pickle\n",
    "filename = \"finalized_InsuranceChargePrediction_RF_Model.sav\"\n",
    "pickle.dump(regressor, open(filename, \"wb\"))\n",
    "\n",
    "# test the model with the saved model file\n",
    "\n",
    "loaded_model_RF = pickle.load(open(\"finalized_InsuranceChargePrediction_RF_Model.sav\", \"rb\"))\n",
    "resultPredictedInsuranceCharges = loaded_model_RF.predict([ [19, 27, 0, 0, 0]]) # 1783.4740471\n",
    "print (\"RF resultPredictedInsuranceCharges 19, 27, 0, 0, 0\" , resultPredictedInsuranceCharges)\n",
    "resultPredictedInsuranceCharges1 = loaded_model_RF.predict([ [19, 26, 0, 0, 0]]) # 16824.917100\n",
    "print (\"RF resultPredictedInsuranceCharges1 19, 26, 0, 0, 0 :\" , resultPredictedInsuranceCharges1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cb177010-7b86-42a5-9091-b3b7f9d9d143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1783.4740471])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultPredictedInsuranceCharges1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9cdc7cf2-eb95-48ac-bc83-e970e9979e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1783.4740471])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultPredictedInsuranceCharges1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62d27ea7-9421-4225-b8c6-0a005ef7045d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>smoker_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>50</td>\n",
       "      <td>30.970</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>18</td>\n",
       "      <td>31.920</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>18</td>\n",
       "      <td>36.850</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>21</td>\n",
       "      <td>25.800</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>61</td>\n",
       "      <td>29.070</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1338 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     bmi  children  sex_male  smoker_yes\n",
       "0      19  27.900         0     False        True\n",
       "1      18  33.770         1      True       False\n",
       "2      28  33.000         3      True       False\n",
       "3      33  22.705         0      True       False\n",
       "4      32  28.880         0      True       False\n",
       "...   ...     ...       ...       ...         ...\n",
       "1333   50  30.970         3      True       False\n",
       "1334   18  31.920         0     False       False\n",
       "1335   18  36.850         0     False       False\n",
       "1336   21  25.800         0     False       False\n",
       "1337   61  29.070         0     False        True\n",
       "\n",
       "[1338 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b509af36-405d-4859-833d-e1bbb4632957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
